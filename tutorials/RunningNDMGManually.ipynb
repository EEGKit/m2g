{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running ndmg : One Subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial provides a basic overview of how to run ndmg manually within Python.\n",
    "The absolute easiest way is to run the pipeline from the command line once all dependencies are installed using the following command:\n",
    "\n",
    "`ndmg_bids </absolute/input/dir> </absolute/output/dir>`.\n",
    "\n",
    "This will run a single session from the input directory, and output the results into your output directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "1. First, we grab some test data and atlases from our github repository. <br>\n",
    "2. Then, we choose our input parameters (the defaults work fine if you don't want to worry about this!) <br>\n",
    "3. Last, we run the pipeline.\n",
    "\n",
    "Running the pipeline is quite simple: call `ndmg_dwi_pipeline.ndmg_dwi_worker` with the correct input flags.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import glob\n",
    "import shutil\n",
    "import warnings\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "from ndmg.scripts import ndmg_dwi_pipeline\n",
    "from ndmg.utils import s3_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download test data, atlases, and check for dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will grab some sample diffusion MRI data from our `neuroparc` repository, as well as the atlases ndmg needs to run. <br>\n",
    "If you want to explore these data and atlases, you can find it in `~/.ndmg/`.\n",
    "\n",
    "Note that the below code requires `git lfs` to be installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure that AFNI and FSL are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your fsl directory is located here: /usr/local/fsl\n",
      "Your AFNI directory is located here: /Users/alex/abin/afni\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FSL\n",
    "try:\n",
    "    print(f\"Your fsl directory is located here: {os.environ['FSLDIR']}\")\n",
    "except KeyError:\n",
    "    raise AssertionError(\"You do not have FSL installed! See installation instructions here: https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FslInstallation\")\n",
    "    \n",
    "# AFNI\n",
    "try:\n",
    "    print(f\"Your AFNI directory is located here: {subprocess.check_output('which afni', shell=True, universal_newlines=True)}\")\n",
    "except subprocess.CalledProcessError:\n",
    "    raise AssertionError(\"You do not have AFNI installed! See installation instructions here: https://afni.nimh.nih.gov/pub/dist/doc/htmldoc/background_install/main_toc.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data directory if it doesn't exist\n",
    "ndmg_dir = Path.home() / \".ndmg/\"\n",
    "data_dir = ndmg_dir / \"data/\"\n",
    "\n",
    "# Remove old data if it exists\n",
    "if data_dir.is_dir():\n",
    "    shutil.rmtree(data_dir)\n",
    "    \n",
    "# Remove neuroparc if it already exists\n",
    "if Path(\"neuroparc\").is_dir():\n",
    "    shutil.rmtree(\"neuroparc\")\n",
    "    \n",
    "# Clone the sample data into `~/.ndmg`.\n",
    "data_dir.mkdir(parents=True)\n",
    "!git lfs clone https://github.com/neurodata/neuroparc.git\n",
    "shutil.move(\"neuroparc/data/BNU1\", data_dir)\n",
    "data_dir = data_dir / \"BNU1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Atlases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove old atlas dir if it exists\n",
    "atlas_dir = ndmg_dir / \"ndmg_atlases\"\n",
    "if atlas_dir.is_dir():\n",
    "    shutil.rmtree(atlas_dir)\n",
    "\n",
    "# Download atlases to atlas_dir\n",
    "atlas_dir.mkdir(parents=True)\n",
    "for name in Path(\"neuroparc\").iterdir():\n",
    "    shutil.move(str(name), atlas_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove our now-empty directory\n",
    "shutil.rmtree(\"neuroparc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose input parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naming Conventions\n",
    "Here, we define input variables to the pipeline.\n",
    "To run the `ndmg` pipeline, you need four files:\n",
    "1. a `t1w` - this is a high-resolution anatomical image.\n",
    "2. a `dwi` - the diffusion image.\n",
    "3. bvecs - this is a text file that defines the gradient vectors created by a DWI scan.\n",
    "4. bvals - this is a text file that defines magnitudes for the gradient vectors created by a DWI scan.\n",
    "\n",
    "The naming convention is in the [BIDs](https://bids.neuroimaging.io/) spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify base directory and paths to input files (dwi, bvecs, bvals, and t1w required)\n",
    "subject_id = 'sub-0025864'\n",
    "\n",
    "# Define the location of our input files.\n",
    "t1w = str(data_dir / f\"{subject_id}/ses-1/anat/{subject_id}_ses-1_T1w.nii.gz\")\n",
    "dwi = str(data_dir / f\"{subject_id}/ses-1/dwi/{subject_id}_ses-1_dwi.nii.gz\")\n",
    "bvecs = str(data_dir / f\"{subject_id}/ses-1/dwi/{subject_id}_ses-1_dwi.bvec\")\n",
    "bvals = str(data_dir / f\"{subject_id}/ses-1/dwi/{subject_id}_ses-1_dwi.bval\")\n",
    "\n",
    "print(t1w)\n",
    "print(dwi)\n",
    "print(bvecs)\n",
    "print(bvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Choices\n",
    "Here, we choose the parameters to run the pipeline with.\n",
    "If you are inexperienced with diffusion MRI theory, feel free to just use the default parameters.\n",
    "\n",
    "- *atlases = ['desikan', 'CPAC200', 'DKT', 'HarvardOxfordcort', 'HarvardOxfordsub', 'JHU', 'Schaefer2018-200', 'Talairach', 'aal', 'brodmann', 'glasser', 'yeo-7-liberal', 'yeo-17-liberal']* : The atlas that defines the node location of the graph you create.\n",
    "- *mod_types = ['det', 'prob']* : Deterministic or probablistic tractography.\n",
    "- *track_types = ['local', 'particle']* : Local or particle tracking.\n",
    "- *mods = ['csa', 'csd']* : [Constant Solid Angle](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4360965/) or [Constrained Spherical Deconvolution](https://onlinelibrary.wiley.com/doi/10.1002/ima.22005).\n",
    "- regs = *['native', 'native_dsn', 'mni']* : Registration style. If native, do all registration in each scan's space; if mni, register scans to the MNI atlas; if native_dsn, do registration in native space, and then fit the streamlines to MNI space.\n",
    "- vox_size = *['1mm', '2mm']* : Whether our voxels are 1mm or 2mm.\n",
    "- seeds = int : Seeding density for tractography. More seeds generally results in a better graph, but at a much higher computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the default parameters.\n",
    "atlas = 'desikan'\n",
    "mod_type = 'det'\n",
    "track_type = 'local'\n",
    "mod_func = 'csd'\n",
    "reg_style = 'native'\n",
    "vox_size = '2mm'\n",
    "seeds = 20\n",
    "\n",
    "# Set an output directory\n",
    "outdir = '/tmp/output_{}_{}_{}_{}_{}_{}_{}'.format(atlas, mod_type, track_type, mod_func, seeds, reg_style, subject_id)\n",
    "print(f\"Your output directory will be : {outdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get masks and labels\n",
    "The pipeline needs these two variables as input. <br>\n",
    "Running the pipeline via `ndmg_bids` does this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-set paths to neuroparc files\n",
    "mask = str(atlas_dir / \"atlases/mask/MNI152NLin6_res-2x2x2_T1w_descr-brainmask.nii.gz\")\n",
    "labels = [str(i) for i in (atlas_dir / \"atlases/label/Human/\").glob(f\"*{atlas}*2x2x2.nii.gz\")]\n",
    "\n",
    "print(f\"mask location : {mask}\")\n",
    "print(f\"atlas location : {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndmg_dwi_pipeline.ndmg_dwi_worker(dwi=dwi, bvals=bvals, bvecs=bvecs, t1w=t1w, atlas=atlas, mask=mask, labels=labels, outdir=outdir, vox_size=vox_size, mod_type=mod_type, track_type=track_type, mod_func=mod_func, seeds=seeds, reg_style=reg_style, clean=False, skipeddy=True, skipreg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import nipype utilities (must install nipype)\n",
    "'''\n",
    "from nipype.pipeline import engine as pe\n",
    "from nipype.interfaces import utility as niu\n",
    "from nipype.interfaces.base import BaseInterface, BaseInterfaceInputSpec, TraitedSpec, File, traits, SimpleInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Construct multi-subject workflow\n",
    "'''\n",
    "\n",
    "wf = pe.Workflow(name=\"NDMG_group_wf\")\n",
    "\n",
    "# Create an import list of modules/objects to permeate the workflow environment\n",
    "import_list = [\"import shutil\", \"import time\", \"import os\", \"import numpy as np\", \"import networkx as nx\",\n",
    "               \"import nibabel as nib\", \"import warnings\", \"warnings.filterwarnings(\\\"ignore\\\")\",\n",
    "               \"np.warnings.filterwarnings(\\\"ignore\\\")\", \"warnings.simplefilter(\\\"ignore\\\")\", \n",
    "               \"from subprocess import Popen\", \"from dipy.tracking.streamline import Streamlines\",\n",
    "              \"import ndmg\", \"from ndmg import preproc as mgp\", \"from ndmg.utils import gen_utils as mgu\", \n",
    "               \"from ndmg.utils import s3_utils\", \"from ndmg.register import gen_reg as mgr\", \n",
    "               \"from ndmg.track import gen_track as mgt\", \"from ndmg.graph import gen_graph as mgg\",\n",
    "              \"from ndmg.utils.bids_utils import name_resource\", \"from ndmg.stats.qa_tensor import *\", \n",
    "               \"from ndmg.stats.qa_fibers import *\", \"from datetime import datetime\"]\n",
    "\n",
    "inputnode = pe.Node(niu.IdentityInterface(fields=['atlas', 'mask', 'labels', \n",
    "                                                  'outdir', 'vox_size', 'mod_type', \n",
    "                                                  'track_type', 'mod_func', 'reg_style', 'clean', \n",
    "                                                  'skedy', 'skipreg']),\n",
    "                    name='inputnode', imports=import_list)\n",
    "\n",
    "inputnode.inputs.atlas = atlas\n",
    "inputnode.inputs.mask = mask\n",
    "inputnode.inputs.labels = labels[0]\n",
    "inputnode.inputs.outdir = outdir\n",
    "inputnode.inputs.vox_size = vox_size\n",
    "inputnode.inputs.mod_type = mod_type\n",
    "inputnode.inputs.track_type = track_type\n",
    "inputnode.inputs.mod_func = mod_func\n",
    "inputnode.inputs.seeds = seeds\n",
    "inputnode.inputs.reg_style = reg_style\n",
    "inputnode.inputs.clean = False\n",
    "inputnode.inputs.skedy = False\n",
    "inputnode.inputs.skipreg = False\n",
    "\n",
    "# Make NDMG an interface\n",
    "class ndmgDWIworkerInputSpec(BaseInterfaceInputSpec):\n",
    "    \"\"\"\n",
    "    Input interface wrapper for ndmgDWIworker\n",
    "    \"\"\"\n",
    "    dwi = traits.Str(mandatory=True)\n",
    "    bvals = traits.Str(mandatory=True)\n",
    "    bvecs = traits.Str(mandatory=True)\n",
    "    t1w = traits.Str(mandatory=True)\n",
    "    atlas = traits.Str(mandatory=True)\n",
    "    mask = traits.Str(mandatory=True)\n",
    "    labels = traits.Str(mandatory=True)\n",
    "    outdir = traits.Str(mandatory=True)\n",
    "    vox_size = traits.Str(mandatory=True)\n",
    "    mod_type = traits.Str(mandatory=True)\n",
    "    track_type = traits.Str(mandatory=True)\n",
    "    mod_func = traits.Str(mandatory=True)\n",
    "    seeds = traits.Str(mandatory=True)\n",
    "    reg_style = traits.Str(mandatory=True)\n",
    "    clean = traits.Bool(mandatory=False)\n",
    "    skedy = traits.Bool(mandatory=False)\n",
    "    skipreg = traits.Bool(mandatory=False)\n",
    "\n",
    "class ndmgDWIworker(BaseInterface):\n",
    "    \"\"\"\n",
    "    Interface wrapper for ndmgDWIworker\n",
    "    \"\"\"\n",
    "    input_spec = ndmgDWIworkerInputSpec\n",
    "\n",
    "    def _run_interface(self, runtime):\n",
    "        out = ndmg_dwi_pipeline.ndmg_dwi_worker(\n",
    "            self.inputs.dwi,\n",
    "            self.inputs.bvals,\n",
    "            self.inputs.bvecs,\n",
    "            self.inputs.t1w,\n",
    "            self.inputs.atlas,\n",
    "            self.inputs.mask,\n",
    "            self.inputs.labels,\n",
    "            self.inputs.outdir,\n",
    "            self.inputs.vox_size,\n",
    "            self.inputs.mod_type,\n",
    "            self.inputs.track_type,\n",
    "            self.inputs.mod_func,\n",
    "            self.inputs.seeds,\n",
    "            self.inputs.reg_style,\n",
    "            self.inputs.clean,\n",
    "            self.inputs.skedy,\n",
    "            self.inputs.skipreg)\n",
    "        setattr(self, '_outpath', out)\n",
    "        return runtime\n",
    "\n",
    "\n",
    "# Make the NDMG wf itself a node of the DAG\n",
    "ndmg_dwi_worker_node = pe.Node(interface=ndmgDWIworker(),\n",
    "                                  inputs=['dwi', 'bvals', 'bvecs', 't1w',\n",
    "                                          'atlas', 'mask', 'labels', \n",
    "                                          'outdir', 'vox_size', 'mod_type', \n",
    "                                          'track_type', 'mod_func', 'seeds', \n",
    "                                          'reg_style', 'clean', 'skedy', 'skipreg'],\n",
    "                                   nested=True, synchronize=True, name='ndmg_dwi_worker_node', imports=import_list)\n",
    "\n",
    "# Restrict cpu and memory for that node such that scheduling will wait for available \n",
    "# resources before proceeding with additional subjects in parallel\n",
    "ndmg_dwi_worker_node.interface.n_procs = 2\n",
    "ndmg_dwi_worker_node.interface.mem_gb = 8\n",
    "\n",
    "# Make subject files iterable\n",
    "ndmg_dwi_worker_node.iterables = [(\"dwi\", [i[4] for i in input_files]), (\"bvals\", [i[2] for i in input_files]), (\"bvecs\", [i[3] for i in input_files]), (\"t1w\", [i[1] for i in input_files])]\n",
    "\n",
    "# Connect variables of workflow\n",
    "wf.connect([\n",
    "    (inputnode, ndmg_dwi_worker_node, [('atlas', 'atlas'), \n",
    "                                       ('mask', 'mask'), \n",
    "                                       ('labels', 'labels'), \n",
    "                                       ('outdir', 'outdir'), \n",
    "                                       ('vox_size', 'vox_size'), \n",
    "                                       ('mod_type', 'mod_type'), \n",
    "                                       ('track_type', 'track_type'), \n",
    "                                       ('mod_func', 'mod_func'), \n",
    "                                       ('seeds', 'seeds'), \n",
    "                                       ('reg_style', 'reg_style'), \n",
    "                                       ('clean', 'clean'), \n",
    "                                       ('skedy', 'skedy'), \n",
    "                                       ('skipreg', 'skipreg')])\n",
    "])\n",
    "\n",
    "# Configure wf execution\n",
    "cfg = dict(execution={'stop_on_first_crash': True, 'hash_method': 'content', 'crashfile_format': 'txt',\n",
    "                      'display_variable': ':0', 'job_finished_timeout': 65, 'matplotlib_backend': 'Agg',\n",
    "                      'use_relative_paths': True, 'parameterize_dirs': False,\n",
    "                      'remove_unnecessary_outputs': False, 'remove_node_directories': False,\n",
    "                      'raise_insufficient': True})\n",
    "for key in cfg.keys():\n",
    "    for setting, value in cfg[key].items():\n",
    "        wf.config[key][setting] = value\n",
    "\n",
    "# Set runtime directory to home directory\n",
    "wf.base_dir = op.expanduser(\"~\")\n",
    "\n",
    "# Run wf using multiproc scheduler\n",
    "wf.run(plugin='MultiProc', plugin_args = {'n_procs': int(2), 'memory_gb': int(16)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
